import requests
import re
import csv
import cloudscraper

# Ù…ØµØ§Ø¯Ø± Ù‚Ù†ÙˆØ§Øª Ø±ÙŠØ§Ø¶ÙŠØ© Ø¹Ø§Ù…Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©
SOURCES = [
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/ar.m3u",
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/s.m3u",
    "https://raw.githubusercontent.com/Free-TV/IPTV/master/playlist.m3u"
]

# ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø±ÙŠØ§Ø¶Ø©
SPORTS_KEYWORDS = ['sport', 'beIN', 'SSC', 'KSA', 'Stadium', 'Abu Dhabi', 'Ø±ÙŠØ§Ø¶Ø©', 'ÙƒØ±Ø©']

def is_token_link(url):
    """ÙØ­Øµ Ø§Ù„Ø±Ø§Ø¨Ø·: Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ù…ÙˆØ² Ø·ÙˆÙŠÙ„Ø© Ø£Ùˆ ØµÙŠØºØ© Ø§Ø´ØªØ±Ø§Ùƒ ÙŠØ±ÙØ¶Ù‡"""
    # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª ØªØ¯Ù„ Ø¹Ù„Ù‰ ØªÙˆÙƒÙ† Ø£Ùˆ Ø§Ø´ØªØ±Ø§Ùƒ
    token_patterns = ['token=', 'key=', 'auth', 'pass', 'user', 'session']
    if any(pattern in url.lower() for pattern in token_patterns):
        return True
    
    # Ø§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø³Ù„Ø§Ø³Ù„ Ù†ØµÙŠØ© Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ (Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©)
    path_segments = url.split('/')
    for segment in path_segments:
        if len(segment) > 20: # Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¹Ø§Ø¯Ø© ØªÙƒÙˆÙ† Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹
            return True
    return False

def check_link(url):
    """ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ø¹Ø§Ù… ÙˆÙŠØ¹Ù…Ù„ Ø¨Ø¯ÙˆÙ† Ø­Ù…Ø§ÙŠØ©"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        with requests.get(url, timeout=5, stream=True, headers=headers) as r:
            if r.status_code == 200:
                return True
        return False
    except:
        return False

def start_process():
    scraper = cloudscraper.create_scraper()
    valid_sports = []
    seen_urls = set()

    print("ğŸš€ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù€ Sports Zone Ø¹Ù† Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙ‚Ø·...")

    for source in SOURCES:
        try:
            response = scraper.get(source, timeout=15)
            # Ù†Ù…Ø· ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ù„Ø±Ø§Ø¨Ø·
            matches = re.findall(r'#EXTINF:.*?,(.*?)\n(http.*?)\n', response.text)

            for name, url in matches:
                url = url.strip()
                name = name.strip()

                # Ø§Ù„Ø´Ø±ÙˆØ·: 
                # 1. Ø§Ø³Ù… Ø±ÙŠØ§Ø¶ÙŠ 
                # 2. Ù„ÙŠØ³ ØªÙˆÙƒÙ† 
                # 3. Ù„Ù… ÙŠØªÙƒØ±Ø±
                if any(key.lower() in name.lower() for key in SPORTS_KEYWORDS):
                    if not is_token_link(url) and url not in seen_urls:
                        
                        if len(valid_sports) < 50:
                            if check_link(url):
                                valid_sports.append({
                                    'title': name,
                                    'url': url
                                })
                                seen_urls.add(url)
                                print(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ©: {name}")
        except:
            continue

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    with open('database.csv', 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['title', 'url'])
        writer.writerows(valid_sports)
    
    print(f"ğŸ ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ«! ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(valid_sports)} Ù‚Ù†Ø§Ø© Ø±ÙŠØ§Ø¶ÙŠØ© Ø¹Ø§Ù…Ø©.")

if __name__ == "__main__":
    start_process()

import requests
import csv
import re
import cloudscraper
import os

# Ù…ØµØ§Ø¯Ø± Ø°Ù‡Ø¨ÙŠØ© Ù…ØªØ¬Ø¯Ø¯Ø© Ù„Ù‚Ù†ÙˆØ§Øª beIN Ùˆ SSC
SOURCES = [
    "https://raw.githubusercontent.com/iptv-org/iptv/master/streams/ar.m3u",
    "https://raw.githubusercontent.com/skid96/M3U/main/Sport.m3u",
    "https://raw.githubusercontent.com/YassinEnnamli/iptv/master/sport.m3u",
    "https://raw.githubusercontent.com/Moebis/beIN-Sports-IPTV/master/beIN.m3u" # Ù…ØµØ¯Ø± Ù…Ø®ØµØµ Ù„Ù€ beIN
]

# ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªÙÙˆÙŠØª Ø£ÙŠ Ù‚Ù†Ø§Ø© Ø±ÙŠØ§Ø¶ÙŠØ© Ø¹Ø±Ø¨ÙŠØ©
SPORTS_KEYWORDS = ['beIN', 'SSC', 'KSA', 'Ø±ÙŠØ§Ø¶Ø©', 'AD Sports', 'Alkass', 'Ø¨ÙŠÙ† Ø³Ø¨ÙˆØ±Øª']
DB_FILE = 'database.csv'

def check_link(url):
    """ÙØ­Øµ Ø³Ø±ÙŠØ¹ ÙˆØµØ§Ø±Ù… Ù„Ù„Ø±Ø§Ø¨Ø· Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        # Ù†Ø³ØªØ®Ø¯Ù… GET Ù…Ø¹ timeout Ù‚ØµÙŠØ± Ø¬Ø¯Ø§Ù‹ Ù„Ø³Ø±Ø¹Ø© Ø§Ù„ÙØ­Øµ
        with requests.get(url, timeout=4, stream=True, headers=headers) as r:
            return r.status_code == 200
    except:
        return False

def start_process():
    scraper = cloudscraper.create_scraper()
    final_list = []
    seen_urls = set()

    # 1. ÙØ­Øµ ÙˆØªØ·Ù‡ÙŠØ± Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ
    if os.path.exists(DB_FILE):
        print("ğŸ” Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ø®Ø²Ù†Ø© Ø­Ø§Ù„ÙŠØ§Ù‹...")
        with open(DB_FILE, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if check_link(row['url']):
                    final_list.append(row)
                    seen_urls.add(row['url'])

    # 2. Ø§Ù„Ù‡Ø¬ÙˆÙ… Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ Ø³Ø­Ø¨ Ø±ÙˆØ§Ø¨Ø· beIN Sports Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©...")
    for source in SOURCES:
        try:
            response = scraper.get(source, timeout=10)
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ù„Ø±Ø§Ø¨Ø· Ø¨Ø¯Ù‚Ø©
            matches = re.findall(r'#EXTINF:.*?,(.*?)\n(http.*?)\n', response.text)
            for name, url in matches:
                url = url.strip()
                name = name.strip()
                
                # Ø´Ø±Ø· Ø§Ù„Ø¥Ø¶Ø§ÙØ©: Ø§Ø³Ù… Ø±ÙŠØ§Ø¶ÙŠØŒ Ù„ÙŠØ³ ØªÙˆÙƒÙ†ØŒ Ù„ÙŠØ³ Ù…ÙƒØ±Ø±ØŒ ÙˆØ´ØºØ§Ù„
                if any(k.lower() in name.lower() for k in SPORTS_KEYWORDS):
                    if "token" not in url.lower() and url not in seen_urls:
                        if check_link(url):
                            final_list.append({'title': name, 'url': url})
                            seen_urls.add(url)
                            print(f"â• Ù…Ø¶Ø§ÙØ© Ø§Ù„Ø¢Ù†: {name}")
        except: continue

    # 3. ØªØ±ØªÙŠØ¨ Ø°ÙƒÙŠ: beIN Sports ØªØ¸Ù‡Ø± ÙÙŠ Ø§Ù„Ù‚Ù…Ø© Ø¯Ø§Ø¦Ù…Ø§Ù‹
    # ÙŠØªÙ… Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¨Ø­ÙŠØ« Ø£ÙŠ Ø§Ø³Ù… ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ beIN ÙŠØ±ØªÙØ¹ Ù„Ù„Ø£Ø¹Ù„Ù‰
    final_list.sort(key=lambda x: ("BEIN" in x['title'].upper()), reverse=True)

    # 4. Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    with open(DB_FILE, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['title', 'url'])
        writer.writeheader()
        writer.writerows(final_list)
    
    print(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ«! Ù„Ø¯ÙŠÙƒ Ø§Ù„Ø¢Ù† {len(final_list)} Ù‚Ù†Ø§Ø© Ø±ÙŠØ§Ø¶ÙŠØ© Ø¬Ø§Ù‡Ø²Ø©.")

if __name__ == "__main__":
    start_process()
